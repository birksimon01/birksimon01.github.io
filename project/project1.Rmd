---
title: 'Project 1: Exploratory Data Analysis'
author: "SDS348"
date: ''
output:
  html_document:
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
  pdf_document:
    toc: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, fig.align = "center", warning = F, message = F,
tidy=TRUE, tidy.opts=list(width.cutoff=60), R.options=list(max.print=100))
```

#Simon Birk SLB4868

## Data Wrangling and Data Exploration

### Instructions
A knitted R Markdown document (ideally HTML) and the raw R Markdown file (as .Rmd) should both be submitted to Canvas by 11:59pm on 10/11/2020. These two documents will be graded jointly, so they must be consistent (i.e., donâ€™t change the R Markdown file without also updating the knitted document).

The text of the document should provide a narrative structure around your code/output. All results presented must have corresponding code. Any answers/results/plots etc. given without the corresponding R code that generated the result will not be considered. Furthermore, all code contained in your final project document must work correctly (knit early, knit often)! Please do not include any extraneous code or code which produces error messages. (Code that produces warnings is acceptable, as long as you understand what the warnings mean!)

### Find data:

Find two (!) datasets with one variable in common (e.g., dates, times, states, counties, countries, sports players), both with at least 50 observations (i.e., rows) in each. Please think very carefully about whether it makes sense to combine your datasets! If you find one dataset with 50 patients and it has their age, and you find another dataset with 50 *different* patients that has their ages, it makes no sense to join them based on age (you would just be pairing up random people of the same age).

When combined, the resulting/final dataset must have **at least 4 different variables (at least 3 numeric) in addition to the common variable** (i.e., five variables total).

You can have as many variables as you would like! If you found two datasets that you like but they don't have enough variables, find a third dataset with the same common variable and join all three.

1. Producing the fund holdings dataset in R using an outer/full join
```{R}
library(tidyverse)

fund_tickers = c('DIA', 'DIM', 'EEM', 'IHI', 'MGK', 'OGIG', 'SMOG', 'SOXX', 'SPY', 'USMV', 'VIG', 'VOO', 'VOX', 'VTI', 'VTV', 'VUG', 'VYM', 'WCLD', 'XLK')

raw_url <- function(ticker) {
  url <- paste("https://raw.githubusercontent.com/birksimon01/data/main/", ticker, "-holdings.csv", sep="")
  return(url)
}

holdings_df <- read_csv("https://raw.githubusercontent.com/birksimon01/data/main/DGRW-holdings.csv", skip=12) %>% mutate(Fund="DGRW")

for (ticker in fund_tickers) {
  holdings_df_add <- read_csv(raw_url(ticker), skip=12) %>% mutate(Fund=ticker)
  holdings_df <- holdings_df %>% full_join(holdings_df_add)
}

#checking to see if it worked
holdings_df %>% group_by(Fund) %>% summarise(num_holdings=n()) %>% head()

#saving csv of all the symbols (taken out for the final knitting) --> using tool to grab more stuff specific to the stock
holdings_df %>% select(-Fund, -Weighting)  %>% unique() %>% arrange(desc(Symbol))

#loading in the stock specific stuff post-tool + omit NAs
stock_info <- read_csv("https://raw.githubusercontent.com/birksimon01/data/main/stocks.csv") %>% na.omit()
```

Grabbing the other two datasets produced by the python script
```{R}
fund_data <- read_csv("https://raw.githubusercontent.com/birksimon01/data/main/fund_data.csv") %>% select(-X1)

portfolio <- read_csv("https://raw.githubusercontent.com/birksimon01/data/main/holdings.csv") %>% select(-X1)
```

### Guidelines

1. If the datasets are not tidy, you will need to reshape them so that every observation has its own row and every variable its own column. If the datasets are both already tidy, you will make them untidy with `pivot_wider()/spread()` and then tidy them again with `pivot_longer/gather()` to demonstrate your use of the functions. It's fine to wait until you have your descriptives to use these functions (e.g., you might want to pivot_wider() to rearrange the data to make your descriptive statistics easier to look at); it's fine long as you use them at least once!

```{R}
#Pivot wide to see the funds vs holdings, attempting to arrange the symbols according to overall fund presence.

#grabbing vector of symbols present in most 
holdings_df %>% group_by(Symbol) %>% summarize(num_funds = n()) %>% arrange(-num_funds) %>% pull(Symbol) -> symbol_vector

#grabbing vector of funds with most holdings
holdings_df %>% group_by(Fund) %>% summarize(num_holdings = n()) %>% arrange(-num_holdings) %>% pull(Fund) -> fund_vector

#sorting columns + rows according to vectors above to be most descriptive
holdings_df %>% select(-Holding)%>%pivot_wider(names_from=Symbol, values_from=Weighting) %>% select(c(Fund, symbol_vector)) %>% arrange(match(Fund, fund_vector)) -> descriptive_wide_df

descriptive_wide_df %>% head()

#retidying
descriptive_wide_df %>% pivot_longer(-Fund, names_to="Symbol", values_to="Weight") %>% mutate(Weight=substring(Weight, 1, nchar(Weight)-1)) %>% mutate(Weight=as.numeric(Weight)) %>% glimpse()
```

2. Join your 2+ separate data sources into a single dataset based on a common ID variable! If you can't find a good pair datasets to join, you may split one main dataset into two different datasets with a common ID variable in each, and then join them back together based on that common ID, but this is obviously less than ideal.

    - You will document the type of join that you do (left/right/inner/full), including a discussion of how many observations were in each dataset, which observations in each dataset were dropped (if any) and why you chose this particular join. 

We are carrying out a right join by the 'Fund' variable, which added AUM and Expense ratio to each observation for a certainfund in the fund_holdings. I chose right join because the fund characteristics should add to fund as extra columns matched by the fund variable. No observations are dropped.
```{R}
#producing full etf dataset using right join. makes no difference right vs. left in this case.
holdings_df %>%right_join(fund_data, by="Fund") -> etf_df 

#another join to add some stock info. Don't believe anything is dropped in this one either. Some of the stocks don't have data though.
etf_df %>% left_join(stock_info, by="Symbol") %>% select(Symbol, Holding, Industry, Marketcap, Weighting, Fund, AUM, Expense_ratio) %>% mutate(Weighting=as.numeric(substring(Weighting, 1, nchar(Weighting)-1))/100)-> etf_df

#the full dataset has been put together, taking data from many sources.
etf_df %>% head()
```

3. Create summary statistics

    - Use *all six* core `dplyr` functions (`filter, select, arrange, group_by, mutate, summarize`) to manipulate and explore your dataset. For mutate, create a  new variable that is a function of at least one other variable, preferably using a dplyr vector function (see dplyr cheatsheet). It's totally fine to use the `_if`, `_at`, `_all` versions of mutate/summarize instead (indeed, it is encouraged if you have lots of variables)
    
Under number 1 was an integrated solution to untidying and retidying that explored a 2d visualization of this dataset, sorting by funds with the most holdings on one axis and symbols present in the highest number of funds on the other. Only one that wasn't used was filter. Let's find out how prevalent stocks in the DIA fund are by using a modified pipeline from part 1.

```{R}
holdings_df %>% group_by(Symbol) %>% summarize(Fund, num_funds = n()) %>% arrange(-num_funds) %>% filter(Fund=="DIA") %>% head()
```

    - Create summary statistics (`mean, sd, var, n, quantile, min, max, n_distinct, cor`, etc) for each of your numeric variables both overall and after grouping by one of your categorical variables (either together or one-at-a-time; if you have two categorical variables, try to include at least one statistic based on a grouping of two categorical variables simultaneously). If you do not have any categorical variables, create one using mutate (e.g., with `case_when` or `ifelse`) to satisfy the `group_by` requirements above. Ideally, you will find a way to show these summary statistics in an easy-to-read table (e.g., by reshaping). (You might explore the kable package for making pretty tables!) If you have lots of numeric variables (e.g., 10+), or your categorical variables have too many categories, just pick a few (either numeric variables or categories of a categorical variable) and summarize based on those. It would be a good idea to show a correlation matrix for your numeric variables (you will need it to make one of your plots).
    
Let's find the average prevalence of stock tickers in each fund compared to every one of the 20 funds in the dataset. It looks like DIA has lots of very common companies, which makes sense given it's supposed to be one of the indices that is representative of US economic performance, using relatively few companies (30) to provide that representation.
    
```{R}
holdings_df %>% 
  group_by(Symbol) %>% 
  summarize(Fund, num_funds = n()) %>% 
  arrange(-num_funds) %>% group_by(Fund) %>% 
  summarize(average_prevalence = mean(num_funds), stdev_prevalence = sd(num_funds), num_holdings = n()) %>%
  arrange(-average_prevalence)
```

Then lets look at the minimum and max weightings to any ticker.
```{R}
etf_df %>% summarize(min_weighting=min(Weighting), max_weighting=max(Weighting))
```
 
4. Make visualizations (three plots)

    -  Make a correlation heatmap of your numeric variables
    
```{R}
library(reshape2)
library(ggplot2)
etf_df %>% rename(ExpenseRatio=Expense_ratio, MarketCap=Marketcap) %>% na.omit() %>% select(MarketCap, Weighting, AUM, ExpenseRatio) %>% cor() -> cor_matrix

NA -> cor_matrix[upper.tri(cor_matrix)]

cor_matrix %>% melt() %>% ggplot(aes(x=Var1, y=Var2, fill=value)) + 
  geom_tile() +labs(x="Variable 1", y="Variable 2", title="Correlation Matrix for Numeric Variables") + theme_classic() + scale_color_gradient2(low='red', high='purple', guide= "colorbar", aesthetics = "fill", midpoint=0)
```

    -  Create at least two additional plots of your choice with ggplot that highlight some of the more interesting features of your data.
    - Each plot (besides the heatmap) should have at least three variables mapped to separate aesthetics
    - Each should use different geoms (don't do two geom_bars)
    - At least one plot should include `stat="summary"`
    - Each plot should include a supporting paragraph describing the relationships that are being visualized and any trends that are apparent
        - It is fine to include more, but limit yourself to 4. Plots should avoid being redundant! Four bad plots will get a lower grade than two good plots, all else being equal.
    - Make them pretty! Use correct labels, etc.
    
Something interesting this plot shows is that there is a pretty clear linear relationship between number of funds a specific stock is present in and the log10-transformed market cap of a company.
```{R}
holdings_df %>% 
  group_by(Symbol) %>% 
  summarize(Fund, num_funds = n()) %>%
  arrange(-num_funds) %>% select(-Fund) %>% 
  unique() %>% filter(Symbol != "CASH") %>% 
  left_join(stock_info, by="Symbol") %>% 
  select(-Name, -Symbol) %>% 
  filter(num_funds > 1) %>% na.omit() %>% 
  ggplot(aes(y=Marketcap, x=num_funds)) + 
  geom_point(color="darkgrey") + 
  scale_y_log10() + geom_smooth(method="lm") + stat_summary(fun=mean, size=5, shape=0, geom="point") +
  labs(x="Number of funds including a public company", y="Market capitalization", title="Relationship between ETF inclusion and market capitalization")
```

Let's see how much equity of the top five industries are owned in these funds and break them down by-fund.

```{R}
etf_df %>% na.omit() %>% group_by(Industry) %>% summarize(total_weight = sum(Weighting)) %>% arrange(-total_weight) %>% slice(1:5) %>% pull(Industry) -> top_industries

etf_df %>% mutate(company_equity=Weighting*AUM*1000) %>% na.omit() %>% subset(Industry %in% top_industries) %>% ggplot(aes(x=Industry, y=company_equity, fill=Fund)) + geom_bar(stat="identity") + theme(axis.text.x = element_text(angle = 45, vjust = 1, 
    size = 10, hjust = 1)) + labs(y="Equity owned by these funds", title="Equity owned by fund in each of the five top sectors")
```

5. Perform k-means/PAM clustering or PCA on (at least) 3 of your numeric variables.

    - Include all steps as we discuss in class, including a visualization.

    - If you don't have at least 3 numeric variables, or you want to cluster based on categorical variables too, convert them to factors in R, generate Gower's dissimilarity matrix on the data, and do PAM clustering on the dissimilarities.
    
    - Show how you chose the final number of clusters/principal components
    
    - Interpret the final clusters/principal components 
    
    - For every step, document what your code does (in words) and what you see in the data!     
    
In order to determine the number of clusters, we first need to find the silhouette widths of the PAM clustering with n clusters, then choose the one with greatest sil_width. We find that to be 4, although 2-5 are essentially the same. Despite this, lowering the cluster number to 2 increases the ease of later steps. 
```{R}
library(cluster)

sil_width <- vector()

for (i in 2:10) {
  pam1 <- etf_df %>% na.omit() %>% select(-Symbol, -Holding, -Industry, -Fund) %>% pam(i)
  sil_width[i]<-pam1$silinfo$avg.width
}
ggplot()+geom_line(aes(x=2:10,y=sil_width[2:10]))
```   

Let's visualize all the combinations when pairing the 4 variables.
```{R}
library(GGally)

pam_optimal <- etf_df %>% na.omit() %>% select(-Symbol, -Holding, -Industry, -Fund) %>% pam(2)

etf_df %>% na.omit() %>% mutate(cluster=as.factor(pam_optimal$clustering)) %>% 
  ggpairs(columns = c("Marketcap","Weighting","AUM","Expense_ratio"), aes(color=cluster))


```
The which=2 plot shows average silhouette width of 1 without showing anything in the graph. Taking that at face value, we've found a strong structure. Not sure why it didn't show the bars. Here is a visualization of the two clusters across the two principal components.
```{R}
plot(pam_optimal, which=1, main="Cluster plot of first 2 components")
```

Here we are getting the eigenvalues for our principal components (directions in which data has the most variance) exploring PCA.
```{R}
#omit NAs, remove non-numerics
etf_df %>% na.omit() %>% select(-Symbol, -Holding, -Industry, -Fund) -> etf_df_prep

#passing correlation matrix into eigen() to get eigenvectors, a feature of that matrix. Eigenvectors point in the direction of the most vector. i.e. your first principal component direction. And second, and third, etc. Our eigenvalues will tell us how much variance exists in each direction.
eig1 <- etf_df_prep %>% cor() %>% eigen()

#scaling our prepped etf dataframe and multiplying by the eigenvector with matrix math to change the coordinate system to match the principal components.
etf_df_prep %>% scale %*% eig1$vectors -> PCAscores

#Here we can see and INTERPRET our PCs. First PC looks for low AUM, high expense ratio, and high weightings(but less so than expense_ratio) what this seems to be singling out is niche ETFs included. Component two looks for high market cap and low weighting, a marker of industry-centered ETFs.
etf_df_prep %>% scale %>% princomp() %>% summary(loadings=T)

#PC1 accounts for 40% (1.6/4) of the variance, PC2 25% (1/4), PC3 ~23% (0.91/4), PC4 ~12% (0.47/4)
eig1$values

etf_df %>% na.omit() %>%mutate(PC1=PCAscores[,1], PC2=PCAscores[,2], cluster=as.factor(pam_optimal$clustering))%>%
  ggplot(aes(PC1,PC2,color=cluster))+geom_point()
```
While most of the variance falls along PC1, it seems that at low PC1 values, PC2 is the determining factor. There seems to be a very characteristic trend for cluster 2. Looking at the data it's a pretty interesting departure between clusters here. Let's investigate a little by changing our plot to color by Fund
```{R}
etf_df %>% na.omit() %>%mutate(PC1=PCAscores[,1], PC2=PCAscores[,2], cluster=as.factor(pam_optimal$clustering)) -> etf_df_mut

etf_df_mut%>%ggplot(aes(PC1,PC2,color=Fund))+geom_point()
```
We find that almost the entirety of cluster 2 that we can see is from EEM, the emerging markets fund, a stark departure from the rest of the funds and the direction that the rest of the funds seems to all align to. Let's see whats common to all by listing out the holdings from the cluster.

```{R}
etf_df_mut %>% na.omit() %>% filter(cluster==2) %>% head()
```
Interestingly enough, we see two things. First, the presence of Tbk PT/Indonesia in the name, and remarkably high market cap. This could be due to an error in marketcap reporting from the tool used to gather this metric (possibly reported in different currency). Not a huge finding (in fact a flaw) but definitely a representation of how PCA/clustering can work to find irregularities and patterns in data without supervision. Had this been a huge dataset and a more real-life scenario, this would definitely inform the project surrounding it on data gathering and provide valuable feedback.

<div class="tocify-extend-page" data-unique="tocify-extend-page" style="height: 0;"></div>

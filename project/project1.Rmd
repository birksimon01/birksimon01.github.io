---
title: 'Project 1: Exploratory Data Analysis'
author: "SDS348"
date: 'October 18, 2020'
output:
  html_document:
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
  pdf_document:
    toc: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, fig.align = "center", warning = F, message = F,
tidy=TRUE, tidy.opts=list(width.cutoff=60), R.options=list(max.print=100))
```

#Simon Birk SLB4868

## Data Wrangling and Data Exploration

1. Producing the fund holdings dataset in R using an outer/full join
```{R}
library(tidyverse)

fund_tickers = c('DIA', 'DIM', 'EEM', 'IHI', 'MGK', 'OGIG', 'SMOG', 'SOXX', 'SPY', 'USMV', 'VIG', 'VOO', 'VOX', 'VTI', 'VTV', 'VUG', 'VYM', 'WCLD', 'XLK')

raw_url <- function(ticker) {
  url <- paste("https://raw.githubusercontent.com/birksimon01/data/main/", ticker, "-holdings.csv", sep="")
  return(url)
}

holdings_df <- read_csv("https://raw.githubusercontent.com/birksimon01/data/main/DGRW-holdings.csv", skip=12) %>% mutate(Fund="DGRW")

for (ticker in fund_tickers) {
  holdings_df_add <- read_csv(raw_url(ticker), skip=12) %>% mutate(Fund=ticker)
  holdings_df <- holdings_df %>% full_join(holdings_df_add)
}

#checking to see if it worked
holdings_df %>% group_by(Fund) %>% summarise(num_holdings=n()) %>% head()

#saving csv of all the symbols (taken out for the final knitting) --> using tool to grab more stuff specific to the stock
holdings_df %>% select(-Fund, -Weighting)  %>% unique() %>% arrange(desc(Symbol))

#loading in the stock specific stuff post-tool + omit NAs
stock_info <- read_csv("https://raw.githubusercontent.com/birksimon01/data/main/stocks.csv") %>% na.omit()
```

Grabbing the other two datasets produced by the python script
```{R}
fund_data <- read_csv("https://raw.githubusercontent.com/birksimon01/data/main/fund_data.csv") %>% select(-X1)

portfolio <- read_csv("https://raw.githubusercontent.com/birksimon01/data/main/holdings.csv") %>% select(-X1)
```

### Guidelines

1. Tidying dataset & reshaping to play with the data presentation 
```{R}
#Pivot wide to see the funds vs holdings, attempting to arrange the symbols according to overall fund presence.

#grabbing vector of symbols present in most 
holdings_df %>% group_by(Symbol) %>% summarize(num_funds = n()) %>% arrange(-num_funds) %>% pull(Symbol) -> symbol_vector

#grabbing vector of funds with most holdings
holdings_df %>% group_by(Fund) %>% summarize(num_holdings = n()) %>% arrange(-num_holdings) %>% pull(Fund) -> fund_vector

#sorting columns + rows according to vectors above to be most descriptive
holdings_df %>% select(-Holding)%>%pivot_wider(names_from=Symbol, values_from=Weighting) %>% select(c(Fund, symbol_vector)) %>% arrange(match(Fund, fund_vector)) -> descriptive_wide_df

descriptive_wide_df %>% head()

#retidying
descriptive_wide_df %>% pivot_longer(-Fund, names_to="Symbol", values_to="Weight") %>% mutate(Weight=substring(Weight, 1, nchar(Weight)-1)) %>% mutate(Weight=as.numeric(Weight)) %>% glimpse()
```

2. Joining the 2+ separate data sources into a single dataset based on a common ID variable!

We are carrying out a right join by the 'Fund' variable, which added AUM and Expense ratio to each observation for a certainfund in the fund_holdings. I chose right join because the fund characteristics should add to fund as extra columns matched by the fund variable. No observations are dropped.
```{R}
#producing full etf dataset using right join. makes no difference right vs. left in this case.
holdings_df %>%right_join(fund_data, by="Fund") -> etf_df 

#another join to add some stock info. Don't believe anything is dropped in this one either. Some of the stocks don't have data though.
etf_df %>% left_join(stock_info, by="Symbol") %>% select(Symbol, Holding, Industry, Marketcap, Weighting, Fund, AUM, Expense_ratio) %>% mutate(Weighting=as.numeric(substring(Weighting, 1, nchar(Weighting)-1))/100)-> etf_df

#the full dataset has been put together, taking data from many sources.
etf_df %>% head()
```

3. Creating summary statistics

Under number 1 was an integrated solution to untidying and retidying that explored a 2d visualization of this dataset, sorting by funds with the most holdings on one axis and symbols present in the highest number of funds on the other. Only one that wasn't used was filter. Let's find out how prevalent stocks in the DIA fund are by using a modified pipeline from part 1.

```{R}
holdings_df %>% group_by(Symbol) %>% summarize(Fund, num_funds = n()) %>% arrange(-num_funds) %>% filter(Fund=="DIA") %>% head()
```

Let's find the average prevalence of stock tickers in each fund compared to every one of the 20 funds in the dataset. It looks like DIA has lots of very common companies, which makes sense given it's supposed to be one of the indices that is representative of US economic performance, using relatively few companies (30) to provide that representation.
    
```{R}
holdings_df %>% 
  group_by(Symbol) %>% 
  summarize(Fund, num_funds = n()) %>% 
  arrange(-num_funds) %>% group_by(Fund) %>% 
  summarize(average_prevalence = mean(num_funds), stdev_prevalence = sd(num_funds), num_holdings = n()) %>%
  arrange(-average_prevalence)
```

Then lets look at the minimum and max weightings to any ticker.
```{R}
etf_df %>% summarize(min_weighting=min(Weighting), max_weighting=max(Weighting))
```
 
4. Making visualizations 

    -  Correlation heatmap of my numeric variables
    
```{R}
library(reshape2)
library(ggplot2)
etf_df %>% rename(ExpenseRatio=Expense_ratio, MarketCap=Marketcap) %>% na.omit() %>% select(MarketCap, Weighting, AUM, ExpenseRatio) %>% cor() -> cor_matrix

NA -> cor_matrix[upper.tri(cor_matrix)]

cor_matrix %>% melt() %>% ggplot(aes(x=Var1, y=Var2, fill=value)) + 
  geom_tile() +labs(x="Variable 1", y="Variable 2", title="Correlation Matrix for Numeric Variables") + theme_classic() + scale_color_gradient2(low='red', high='purple', guide= "colorbar", aesthetics = "fill", midpoint=0)
```

Something interesting this plot shows is that there is a pretty clear linear relationship between number of funds a specific stock is present in and the log10-transformed market cap of a company.
```{R}
holdings_df %>% 
  group_by(Symbol) %>% 
  summarize(Fund, num_funds = n()) %>%
  arrange(-num_funds) %>% select(-Fund) %>% 
  unique() %>% filter(Symbol != "CASH") %>% 
  left_join(stock_info, by="Symbol") %>% 
  select(-Name, -Symbol) %>% 
  filter(num_funds > 1) %>% na.omit() %>% 
  ggplot(aes(y=Marketcap, x=num_funds)) + 
  geom_point(color="darkgrey") + 
  scale_y_log10() + geom_smooth(method="lm") + stat_summary(fun=mean, size=5, shape=0, geom="point") +
  labs(x="Number of funds including a public company", y="Market capitalization", title="Relationship between ETF inclusion and market capitalization")
```

Let's see how much equity of the top five industries are owned in these funds and break them down by-fund.

```{R}
etf_df %>% na.omit() %>% group_by(Industry) %>% summarize(total_weight = sum(Weighting)) %>% arrange(-total_weight) %>% slice(1:5) %>% pull(Industry) -> top_industries

etf_df %>% mutate(company_equity=Weighting*AUM*1000) %>% na.omit() %>% subset(Industry %in% top_industries) %>% ggplot(aes(x=Industry, y=company_equity, fill=Fund)) + geom_bar(stat="identity") + theme(axis.text.x = element_text(angle = 45, vjust = 1, 
    size = 10, hjust = 1)) + labs(y="Equity owned by these funds", title="Equity owned by fund in each of the five top sectors")
```

5. Performing k-means/PAM clustering and PCA

First, in order to determine the number of clusters, we first need to find the silhouette widths of the PAM clustering with n clusters, then choose the one with greatest sil_width. We find that to be 4, although 2-5 are essentially the same. Despite this, lowering the cluster number to 2 increases the ease of later steps. 
```{R}
library(cluster)

sil_width <- vector()

for (i in 2:10) {
  pam1 <- etf_df %>% na.omit() %>% select(-Symbol, -Holding, -Industry, -Fund) %>% pam(i)
  sil_width[i]<-pam1$silinfo$avg.width
}
ggplot()+geom_line(aes(x=2:10,y=sil_width[2:10]))
```   

Let's visualize all the combinations when pairing the 4 variables.
```{R}
library(GGally)

pam_optimal <- etf_df %>% na.omit() %>% select(-Symbol, -Holding, -Industry, -Fund) %>% pam(2)

etf_df %>% na.omit() %>% mutate(cluster=as.factor(pam_optimal$clustering)) %>% 
  ggpairs(columns = c("Marketcap","Weighting","AUM","Expense_ratio"), aes(color=cluster))


```
The which=2 plot shows average silhouette width of 1 without showing anything in the graph. Taking that at face value, we've found a strong structure. Not sure why it didn't show the bars. Here is a visualization of the two clusters across the two principal components.
```{R}
plot(pam_optimal, which=1, main="Cluster plot of first 2 components")
```

Here we are getting the eigenvalues for our principal components (directions in which data has the most variance) exploring PCA.
```{R}
#omit NAs, remove non-numerics
etf_df %>% na.omit() %>% select(-Symbol, -Holding, -Industry, -Fund) -> etf_df_prep

#passing correlation matrix into eigen() to get eigenvectors, a feature of that matrix. Eigenvectors point in the direction of the most vector. i.e. your first principal component direction. And second, and third, etc. Our eigenvalues will tell us how much variance exists in each direction.
eig1 <- etf_df_prep %>% cor() %>% eigen()

#scaling our prepped etf dataframe and multiplying by the eigenvector with matrix math to change the coordinate system to match the principal components.
etf_df_prep %>% scale %*% eig1$vectors -> PCAscores

#Here we can see and INTERPRET our PCs. First PC looks for low AUM, high expense ratio, and high weightings(but less so than expense_ratio) what this seems to be singling out is niche ETFs included. Component two looks for high market cap and low weighting, a marker of industry-centered ETFs.
etf_df_prep %>% scale %>% princomp() %>% summary(loadings=T)

#PC1 accounts for 40% (1.6/4) of the variance, PC2 25% (1/4), PC3 ~23% (0.91/4), PC4 ~12% (0.47/4)
eig1$values

etf_df %>% na.omit() %>%mutate(PC1=PCAscores[,1], PC2=PCAscores[,2], cluster=as.factor(pam_optimal$clustering))%>%
  ggplot(aes(PC1,PC2,color=cluster))+geom_point()
```
While most of the variance falls along PC1, it seems that at low PC1 values, PC2 is the determining factor. There seems to be a very characteristic trend for cluster 2. Looking at the data it's a pretty interesting departure between clusters here. Let's investigate a little by changing our plot to color by Fund
```{R}
etf_df %>% na.omit() %>%mutate(PC1=PCAscores[,1], PC2=PCAscores[,2], cluster=as.factor(pam_optimal$clustering)) -> etf_df_mut

etf_df_mut%>%ggplot(aes(PC1,PC2,color=Fund))+geom_point()
```
We find that almost the entirety of cluster 2 that we can see is from EEM, the emerging markets fund, a stark departure from the rest of the funds and the direction that the rest of the funds seems to all align to. Let's see whats common to all by listing out the holdings from the cluster.

```{R}
etf_df_mut %>% na.omit() %>% filter(cluster==2) %>% head()
```
Interestingly enough, we see two things. First, the presence of Tbk PT/Indonesia in the name, and remarkably high market cap. This could be due to an error in marketcap reporting from the tool used to gather this metric (possibly reported in different currency). Not a huge finding (in fact a flaw) but definitely a representation of how PCA/clustering can work to find irregularities and patterns in data without supervision. Had this been a huge dataset and a more real-life scenario, this would definitely inform the project surrounding it on data gathering and provide valuable feedback.

<div class="tocify-extend-page" data-unique="tocify-extend-page" style="height: 0;"></div>
